{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d8e80f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6fee0b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4a3c9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state,action):\n",
    "        super(DQN,self).__init__()\n",
    "        self.input=nn.Linear(state,128)\n",
    "        self.hidden=nn.Linear(128,128)\n",
    "        self.output=nn.Linear(128,action)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.input(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.hidden(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81b5abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replaymemory:\n",
    "    def __init__(self,maxlen):\n",
    "        self.maxlen=maxlen\n",
    "        self.memory=deque([],maxlen=self.maxlen)\n",
    "    \n",
    "    def append(self,transition):\n",
    "        self.memory.append(transition)\n",
    "    \n",
    "    def sample(self,sample_size):\n",
    "        return random.sample(self.memory,sample_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d5880eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameters:\n",
    "    epsilon=1.0\n",
    "    epsilon_end=0.01\n",
    "    epsilon_dec=0.9954\n",
    "\n",
    "    batch_size=32\n",
    "    memory_size=100000\n",
    "    target_update_freq=100\n",
    "\n",
    "    learning_rate=0.0001\n",
    "    discount_factor=0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3299bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self,is_train=True,DDQN=True):\n",
    "        if is_train:\n",
    "            hp=Hyperparameters()\n",
    "            self.epsilon=hp.epsilon\n",
    "            self.epsilon_dec=hp.epsilon_dec\n",
    "            self.epsilon_end=hp.epsilon_end\n",
    "            self.batch_size=hp.batch_size\n",
    "            self.memory_size=hp.memory_size\n",
    "            self.target_update_freq=hp.target_update_freq\n",
    "            self.lossfn=nn.SmoothL1Loss()\n",
    "            self.optimizer=None\n",
    "            self.learning_rate=hp.learning_rate\n",
    "            self.discount_factor=hp.discount_factor\n",
    "\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            self.env = gym.make(\"LunarLander-v3\")\n",
    "            obs_shape = self.env.observation_space.shape[0]\n",
    "            n_actions = self.env.action_space.n\n",
    "\n",
    "            self.policy_net = DQN(obs_shape, n_actions).to(self.device)\n",
    "            self.target_net = DQN(obs_shape, n_actions).to(self.device)\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            self.target_net.eval()\n",
    "\n",
    "            self.memory = Replaymemory(self.memory_size)\n",
    "            self.DDQN=DDQN\n",
    "\n",
    "    def run(self,is_train=True,render=False,episodes=100,DDQN=True,model_path=None):\n",
    "        env=gym.make(\"LunarLander-v3\",render_mode=\"human\" if render else None)\n",
    "        #env=gym.make(\"CartPole-v1\",render_mode=\"human\")\n",
    "\n",
    "        policy_net=self.policy_net\n",
    "\n",
    "        if is_train:\n",
    "            memory=self.memory\n",
    "            Target_net=self.target_net\n",
    "            Target_net.load_state_dict(policy_net.state_dict())\n",
    "            self.optimizer=torch.optim.Adam(policy_net.parameters(),lr=self.learning_rate)\n",
    "            stepcount=0\n",
    "        else:\n",
    "            if model_path is None:\n",
    "                policy_net.load_state_dict(torch.load(r\"D:\\Data Science and ML\\Reinforcement Learning\\Midterm Submission\\lunarlander_dqn.pt\",map_location=self.device))\n",
    "                policy_net.eval()\n",
    "            else:\n",
    "                policy_net.load_state_dict(torch.load(model_path,map_location=self.device))\n",
    "                policy_net.eval()\n",
    "\n",
    "        reward_list=[]\n",
    "        reward_list_mean=[]\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state_main=env.reset()[0]\n",
    "            reward_ep=0\n",
    "            while True:\n",
    "                term=False\n",
    "                state=torch.tensor(state_main,dtype=torch.float32,device=self.device).unsqueeze(0)\n",
    "\n",
    "                if np.random.random()<self.epsilon and is_train:\n",
    "                    action=env.action_space.sample()    \n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        action=policy_net(state).argmax().item()\n",
    "\n",
    "                new_state,reward_move,term,trunc,_=env.step(action)\n",
    "                reward_ep+=reward_move\n",
    "                done=term or trunc\n",
    "                state_main=new_state\n",
    "\n",
    "                if is_train:\n",
    "                    action_tens=torch.tensor([action],dtype=torch.int64,device=self.device)\n",
    "                    new_state_tens=torch.tensor(new_state,dtype=torch.float32,device=self.device).unsqueeze(0)\n",
    "                    reward_tens=torch.tensor(reward_move,dtype=torch.float32,device=self.device)\n",
    "                    term_tens=torch.tensor(term,dtype=torch.float32,device=self.device)\n",
    "                    memory.append((state,action_tens,new_state_tens,reward_tens,term_tens))\n",
    "                    stepcount+=1\n",
    "\n",
    "\n",
    "                if is_train and len(memory)>=self.batch_size:\n",
    "                    batch=memory.sample(self.batch_size)\n",
    "                    self.optimize(batch,policy_net,Target_net,DDQN)\n",
    "                    if stepcount%self.target_update_freq==0:\n",
    "                        Target_net.load_state_dict(policy_net.state_dict())\n",
    "                        stepcount=0\n",
    "\n",
    "                if done :\n",
    "                    break\n",
    "\n",
    "            if self.epsilon>self.epsilon_end and is_train:\n",
    "                self.epsilon*=self.epsilon_dec\n",
    "            else:\n",
    "                self.epsilon=self.epsilon_end\n",
    "            reward_list.append(reward_ep)\n",
    "\n",
    "            if (np.mean(reward_list[-10:])>=200 and (np.mean(reward_list[-10:])>=(reward_list_mean[-1] if reward_list_mean else True))) and is_train :\n",
    "                reward_list_mean.append(np.mean(reward_list[-4:]))\n",
    "                torch.save(policy_net.state_dict(),\"dqn_lunarlander_best.pth\")\n",
    "                print(f\"Updated in episode {i+1}\")\n",
    "            \n",
    "            print(f\"Episode: {i+1}, Reward: {reward_ep}, Epsilon: {self.epsilon}\")\n",
    "        return reward_list\n",
    "    \n",
    "    def optimize(self,batch,policy_net,Target_net,DDQN=True):\n",
    "\n",
    "        states,actions,new_state,reward,term=zip(*batch)\n",
    "\n",
    "        states=torch.cat(states)\n",
    "        actions=torch.stack(actions).view(-1,1).long()\n",
    "        new_state=torch.cat(new_state)\n",
    "        reward=torch.stack(reward).unsqueeze(1)\n",
    "        term=torch.tensor(term,dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        curr_q=policy_net(states).gather(1,actions)\n",
    "        \n",
    "        if DDQN:\n",
    "            with torch.no_grad():\n",
    "                best_nxt_act=policy_net(new_state).argmax(1).unsqueeze(1)\n",
    "                tar_q_val=Target_net(new_state).gather(1,best_nxt_act).squeeze(1)\n",
    "                tar_q=reward+(1-term)*self.discount_factor*tar_q_val\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                tar_q_val=Target_net(new_state).max(1)[0]\n",
    "                tar_q=reward+(1-term)*self.discount_factor*tar_q_val.unsqueeze(1)\n",
    "\n",
    "        loss=self.lossfn(curr_q,tar_q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent=Agent(is_train=True) \n",
    "\n",
    "train_rewards=my_agent.run(is_train=True,render=False,episodes=1000,DDQN=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "254200df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(my_agent.policy_net.state_dict(),\"lunarlander_dqn.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4abde09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 228.9654320216352, Epsilon: 0.01\n",
      "Episode: 2, Reward: 240.5855753099405, Epsilon: 0.01\n",
      "Episode: 3, Reward: 273.16228421625084, Epsilon: 0.01\n",
      "Episode: 4, Reward: 267.93088403329654, Epsilon: 0.01\n",
      "Episode: 5, Reward: 255.16356367525353, Epsilon: 0.01\n",
      "Episode: 6, Reward: 258.6962581653863, Epsilon: 0.01\n",
      "Episode: 7, Reward: 229.08476278498898, Epsilon: 0.01\n",
      "Episode: 8, Reward: 269.2746911755635, Epsilon: 0.01\n",
      "Episode: 9, Reward: 252.4660003509502, Epsilon: 0.01\n",
      "Episode: 10, Reward: 260.24100509464995, Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "test_rewards=my_agent.run(is_train=False,render=True,episodes=10,DDQN=False,model_path=r\"D:\\Data Science and ML\\Reinforcement Learning\\Midterm Submission\\lunarlander_dqn.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
