{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8e80f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fee0b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a3c9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state,action):\n",
    "        super(DQN,self).__init__()\n",
    "        self.input=nn.Linear(state,128)\n",
    "        self.hidden=nn.Linear(128,128)\n",
    "        self.output=nn.Linear(128,action)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.input(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.hidden(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81b5abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replaymemory:\n",
    "    def __init__(self,maxlen):\n",
    "        self.maxlen=maxlen\n",
    "        self.memory=deque([],maxlen=self.maxlen)\n",
    "    \n",
    "    def append(self,transition):\n",
    "        self.memory.append(transition)\n",
    "    \n",
    "    def sample(self,sample_size):\n",
    "        return random.sample(self.memory,sample_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5880eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameters:\n",
    "    epsilon=1.0\n",
    "    epsilon_end=0.01\n",
    "    epsilon_dec=0.992\n",
    "\n",
    "    batch_size=64\n",
    "    memory_size=10000\n",
    "    target_update_freq=1000\n",
    "\n",
    "    learning_rate=0.005\n",
    "    discount_factor=0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb3299bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self,is_train=True,DDQN=True):\n",
    "        if is_train:\n",
    "            hp=Hyperparameters()\n",
    "            self.epsilon=hp.epsilon\n",
    "            self.epsilon_dec=hp.epsilon_dec\n",
    "            self.epsilon_end=hp.epsilon_end\n",
    "            self.batch_size=hp.batch_size\n",
    "            self.memory_size=hp.memory_size\n",
    "            self.target_update_freq=hp.target_update_freq\n",
    "            self.lossfn=nn.SmoothL1Loss()\n",
    "            self.optimizer=None\n",
    "            self.learning_rate=hp.learning_rate\n",
    "            self.discount_factor=hp.discount_factor\n",
    "\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            self.env = gym.make(\"LunarLander-v3\")\n",
    "            obs_shape = self.env.observation_space.shape[0]\n",
    "            n_actions = self.env.action_space.n\n",
    "\n",
    "            self.policy_net = DQN(obs_shape, n_actions).to(self.device)\n",
    "            self.target_net = DQN(obs_shape, n_actions).to(self.device)\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            self.target_net.eval()\n",
    "\n",
    "            self.memory = Replaymemory(self.memory_size)\n",
    "            self.DDQN=DDQN\n",
    "\n",
    "    def run(self,is_train=True,render=False,episodes=100,DDQN=True):\n",
    "        env=gym.make(\"LunarLander-v3\",render_mode=\"human\" if render else None)\n",
    "        #env=gym.make(\"CartPole-v1\",render_mode=\"human\")\n",
    "\n",
    "        policy_net=self.policy_net\n",
    "\n",
    "        if is_train:\n",
    "            memory=self.memory\n",
    "            Target_net=self.target_net\n",
    "            Target_net.load_state_dict(policy_net.state_dict())\n",
    "            self.optimizer=torch.optim.Adam(policy_net.parameters(),lr=self.learning_rate)\n",
    "            stepcount=0\n",
    "\n",
    "        reward_list=[]\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state_main=env.reset()[0]\n",
    "            reward_ep=0\n",
    "            while True:\n",
    "                term=False\n",
    "                state=torch.tensor(state_main,dtype=torch.float32,device=self.device).unsqueeze(0)\n",
    "\n",
    "                if np.random.random()<self.epsilon and is_train:\n",
    "                    action=env.action_space.sample()    \n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        action=policy_net(state).argmax().item()\n",
    "\n",
    "                new_state,reward_move,term,trunc,_=env.step(action)\n",
    "                reward_ep+=reward_move\n",
    "                done=term or trunc\n",
    "                state_main=new_state\n",
    "\n",
    "                if is_train:\n",
    "                    action_tens=torch.tensor([action],dtype=torch.int64,device=self.device)\n",
    "                    new_state_tens=torch.tensor(new_state,dtype=torch.float32,device=self.device).unsqueeze(0)\n",
    "                    reward_tens=torch.tensor(reward_move,dtype=torch.float32,device=self.device)\n",
    "                    term_tens=torch.tensor(term,dtype=torch.float32,device=self.device)\n",
    "                    memory.append((state,action_tens,new_state_tens,reward_tens,term_tens))\n",
    "                    stepcount+=1\n",
    "\n",
    "\n",
    "                if is_train and len(memory)>=self.batch_size:\n",
    "                    batch=memory.sample(self.batch_size)\n",
    "                    self.optimize(batch,policy_net,Target_net,self.DDQN)\n",
    "                    if stepcount%self.target_update_freq==0:\n",
    "                        Target_net.load_state_dict(policy_net.state_dict())\n",
    "                        stepcount=0\n",
    "\n",
    "                if done :\n",
    "                    break\n",
    "\n",
    "            if self.epsilon>self.epsilon_end and is_train:\n",
    "                self.epsilon*=self.epsilon_dec\n",
    "            else:\n",
    "                self.epsilon=self.epsilon_end\n",
    "            reward_list.append(reward_ep)\n",
    "            \n",
    "            print(f\"Episode: {i+1}, Reward: {reward_ep}, Epsilon: {self.epsilon}\")\n",
    "        return reward_list\n",
    "    \n",
    "    def optimize(self,batch,policy_net,Target_net,DDQN=True):\n",
    "\n",
    "        states,actions,new_state,reward,term=zip(*batch)\n",
    "\n",
    "        states=torch.cat(states)\n",
    "        actions=torch.stack(actions).view(-1,1).long()\n",
    "        new_state=torch.cat(new_state)\n",
    "        reward=torch.stack(reward).unsqueeze(1)\n",
    "        term=torch.tensor(term,dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        curr_q=policy_net(states).gather(1,actions)\n",
    "        \n",
    "        if DDQN:\n",
    "            with torch.no_grad():\n",
    "                best_nxt_act=policy_net(new_state).argmax(1).unsqueeze(1)\n",
    "                tar_q_val=Target_net(new_state).gather(1,best_nxt_act).squeeze(1)\n",
    "                tar_q=reward+(1-term)*self.discount_factor*tar_q_val\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                tar_q_val=Target_net(new_state).max(1)[0]\n",
    "                tar_q=reward+(1-term)*self.discount_factor*tar_q_val.unsqueeze(1)\n",
    "\n",
    "        loss=self.lossfn(curr_q,tar_q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a5a047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: -357.94041736143106, Epsilon: 0.992\n",
      "Episode: 2, Reward: -189.69902276709928, Epsilon: 0.9840639999999999\n",
      "Episode: 3, Reward: -201.48500362980647, Epsilon: 0.9761914879999999\n",
      "Episode: 4, Reward: -66.1271266161129, Epsilon: 0.9683819560959999\n",
      "Episode: 5, Reward: -202.43978479832901, Epsilon: 0.9606349004472319\n",
      "Episode: 6, Reward: -240.04715886525332, Epsilon: 0.952949821243654\n",
      "Episode: 7, Reward: -135.5073724811097, Epsilon: 0.9453262226737048\n",
      "Episode: 8, Reward: -116.87531463991535, Epsilon: 0.9377636128923151\n",
      "Episode: 9, Reward: -86.15990272265552, Epsilon: 0.9302615039891766\n",
      "Episode: 10, Reward: -88.34747320042256, Epsilon: 0.9228194119572632\n",
      "Episode: 11, Reward: -130.2701038070507, Epsilon: 0.9154368566616051\n",
      "Episode: 12, Reward: -72.46089854024613, Epsilon: 0.9081133618083123\n",
      "Episode: 13, Reward: -173.49400826912412, Epsilon: 0.9008484549138458\n",
      "Episode: 14, Reward: -165.79386429498663, Epsilon: 0.893641667274535\n",
      "Episode: 15, Reward: -276.67923515253017, Epsilon: 0.8864925339363386\n",
      "Episode: 16, Reward: -106.66799835815486, Epsilon: 0.8794005936648479\n",
      "Episode: 17, Reward: -141.9444171978006, Epsilon: 0.8723653889155292\n",
      "Episode: 18, Reward: -80.2377627952147, Epsilon: 0.865386465804205\n",
      "Episode: 19, Reward: -92.57316173579356, Epsilon: 0.8584633740777713\n",
      "Episode: 20, Reward: -99.51780112555495, Epsilon: 0.8515956670851491\n",
      "Episode: 21, Reward: -107.27622539998002, Epsilon: 0.8447829017484679\n",
      "Episode: 22, Reward: -177.71676959185066, Epsilon: 0.8380246385344802\n",
      "Episode: 23, Reward: -234.42031109985197, Epsilon: 0.8313204414262043\n",
      "Episode: 24, Reward: -88.80528631503951, Epsilon: 0.8246698778947946\n",
      "Episode: 25, Reward: -291.7467685403012, Epsilon: 0.8180725188716362\n",
      "Episode: 26, Reward: -166.47984631533433, Epsilon: 0.8115279387206631\n",
      "Episode: 27, Reward: -76.77541072912172, Epsilon: 0.8050357152108978\n",
      "Episode: 28, Reward: -103.05074285048543, Epsilon: 0.7985954294892106\n",
      "Episode: 29, Reward: -111.72035356694012, Epsilon: 0.7922066660532969\n",
      "Episode: 30, Reward: -170.78139311932253, Epsilon: 0.7858690127248705\n",
      "Episode: 31, Reward: -77.35144344771784, Epsilon: 0.7795820606230716\n",
      "Episode: 32, Reward: -277.3468562186547, Epsilon: 0.773345404138087\n",
      "Episode: 33, Reward: -210.053418980892, Epsilon: 0.7671586409049822\n",
      "Episode: 34, Reward: -109.12678460492207, Epsilon: 0.7610213717777423\n",
      "Episode: 35, Reward: -25.202003811805554, Epsilon: 0.7549332008035203\n",
      "Episode: 36, Reward: -186.22236761660932, Epsilon: 0.7488937351970921\n",
      "Episode: 37, Reward: -121.40742316409953, Epsilon: 0.7429025853155153\n",
      "Episode: 38, Reward: -67.21498943876645, Epsilon: 0.7369593646329912\n",
      "Episode: 39, Reward: -106.03141130714899, Epsilon: 0.7310636897159273\n",
      "Episode: 40, Reward: -273.6802723171746, Epsilon: 0.7252151801981999\n",
      "Episode: 41, Reward: -59.53298835049371, Epsilon: 0.7194134587566142\n",
      "Episode: 42, Reward: -45.62433422882857, Epsilon: 0.7136581510865613\n",
      "Episode: 43, Reward: -177.6514245890635, Epsilon: 0.7079488858778687\n",
      "Episode: 44, Reward: -172.5138841426075, Epsilon: 0.7022852947908458\n",
      "Episode: 45, Reward: -149.80474911155696, Epsilon: 0.696667012432519\n",
      "Episode: 46, Reward: -106.75951578079733, Epsilon: 0.6910936763330588\n",
      "Episode: 47, Reward: -134.21178108814277, Epsilon: 0.6855649269223943\n",
      "Episode: 48, Reward: -60.16233621708264, Epsilon: 0.6800804075070151\n",
      "Episode: 49, Reward: -91.84425346223989, Epsilon: 0.674639764246959\n",
      "Episode: 50, Reward: -26.6229617345992, Epsilon: 0.6692426461329833\n",
      "Episode: 51, Reward: -282.26204662605267, Epsilon: 0.6638887049639195\n",
      "Episode: 52, Reward: -286.76038271685866, Epsilon: 0.6585775953242081\n",
      "Episode: 53, Reward: -211.99193798252128, Epsilon: 0.6533089745616144\n",
      "Episode: 54, Reward: -99.48714498302343, Epsilon: 0.6480825027651215\n",
      "Episode: 55, Reward: -90.15012580966282, Epsilon: 0.6428978427430005\n",
      "Episode: 56, Reward: -381.4794978708045, Epsilon: 0.6377546600010565\n",
      "Episode: 57, Reward: -88.66108621172035, Epsilon: 0.632652622721048\n",
      "Episode: 58, Reward: -140.41626236131611, Epsilon: 0.6275914017392796\n",
      "Episode: 59, Reward: -168.21831386704133, Epsilon: 0.6225706705253654\n",
      "Episode: 60, Reward: -23.9818737403142, Epsilon: 0.6175901051611625\n",
      "Episode: 61, Reward: -37.24587835517548, Epsilon: 0.6126493843198733\n",
      "Episode: 62, Reward: -356.62112224179, Epsilon: 0.6077481892453143\n",
      "Episode: 63, Reward: -22.30374324004906, Epsilon: 0.6028862037313517\n",
      "Episode: 64, Reward: -114.4446403461739, Epsilon: 0.5980631141015009\n",
      "Episode: 65, Reward: -91.48208418688961, Epsilon: 0.5932786091886889\n",
      "Episode: 66, Reward: -268.3547865429048, Epsilon: 0.5885323803151794\n",
      "Episode: 67, Reward: -126.99582205638029, Epsilon: 0.5838241212726579\n",
      "Episode: 68, Reward: 3.5168593536018733, Epsilon: 0.5791535283024767\n",
      "Episode: 69, Reward: -111.87847362166598, Epsilon: 0.5745203000760568\n",
      "Episode: 70, Reward: -66.8173424825828, Epsilon: 0.5699241376754484\n",
      "Episode: 71, Reward: -146.9602658917255, Epsilon: 0.5653647445740447\n",
      "Episode: 72, Reward: -156.89134973756495, Epsilon: 0.5608418266174524\n",
      "Episode: 73, Reward: -73.89174656937762, Epsilon: 0.5563550920045128\n",
      "Episode: 74, Reward: -156.16051418103945, Epsilon: 0.5519042512684768\n",
      "Episode: 75, Reward: -24.50221266336277, Epsilon: 0.5474890172583289\n",
      "Episode: 76, Reward: -77.24145854971403, Epsilon: 0.5431091051202622\n",
      "Episode: 77, Reward: -82.7937057054469, Epsilon: 0.5387642322793001\n",
      "Episode: 78, Reward: -77.46954576643581, Epsilon: 0.5344541184210657\n",
      "Episode: 79, Reward: -28.635042485052935, Epsilon: 0.5301784854736972\n",
      "Episode: 80, Reward: -73.22255510839017, Epsilon: 0.5259370575899076\n",
      "Episode: 81, Reward: -54.5978636479434, Epsilon: 0.5217295611291883\n",
      "Episode: 82, Reward: -112.08125007500709, Epsilon: 0.5175557246401549\n",
      "Episode: 83, Reward: -105.2653547267005, Epsilon: 0.5134152788430336\n",
      "Episode: 84, Reward: -123.69920549124765, Epsilon: 0.5093079566122893\n",
      "Episode: 85, Reward: -97.89718110930971, Epsilon: 0.505233492959391\n",
      "Episode: 86, Reward: 50.905382954243464, Epsilon: 0.5011916250157159\n",
      "Episode: 87, Reward: -179.08332890138797, Epsilon: 0.49718209201559016\n",
      "Episode: 88, Reward: -45.25529968487132, Epsilon: 0.49320463527946545\n",
      "Episode: 89, Reward: -93.11317329039532, Epsilon: 0.48925899819722973\n",
      "Episode: 90, Reward: -207.42630905331006, Epsilon: 0.4853449262116519\n",
      "Episode: 91, Reward: -56.27406203852079, Epsilon: 0.48146216680195864\n",
      "Episode: 92, Reward: -247.0104557575219, Epsilon: 0.477610469467543\n",
      "Episode: 93, Reward: -80.07643246350666, Epsilon: 0.47378958571180263\n",
      "Episode: 94, Reward: -89.49141037977105, Epsilon: 0.4699992690261082\n",
      "Episode: 95, Reward: -27.770922904449776, Epsilon: 0.4662392748738994\n",
      "Episode: 96, Reward: -71.87122529817519, Epsilon: 0.46250936067490817\n",
      "Episode: 97, Reward: -1.47705983026313, Epsilon: 0.4588092857895089\n",
      "Episode: 98, Reward: -27.73852413147928, Epsilon: 0.45513881150319285\n",
      "Episode: 99, Reward: -73.12788179126, Epsilon: 0.4514977010111673\n",
      "Episode: 100, Reward: -7.279212618684255, Epsilon: 0.447885719403078\n",
      "Episode: 101, Reward: -50.36366413349711, Epsilon: 0.44430263364785333\n",
      "Episode: 102, Reward: -117.71702566442262, Epsilon: 0.4407482125786705\n",
      "Episode: 103, Reward: -232.74709446776794, Epsilon: 0.43722222687804113\n",
      "Episode: 104, Reward: -42.78724706087675, Epsilon: 0.4337244490630168\n",
      "Episode: 105, Reward: -112.75999770693171, Epsilon: 0.43025465347051267\n",
      "Episode: 106, Reward: -3.7641570452923503, Epsilon: 0.4268126162427486\n",
      "Episode: 107, Reward: -210.77016876312467, Epsilon: 0.4233981153128066\n",
      "Episode: 108, Reward: -25.18709957223804, Epsilon: 0.4200109303903041\n",
      "Episode: 109, Reward: -118.15796571956935, Epsilon: 0.4166508429471817\n",
      "Episode: 110, Reward: -71.29170698429485, Epsilon: 0.41331763620360423\n",
      "Episode: 111, Reward: -127.22211997377023, Epsilon: 0.4100110951139754\n",
      "Episode: 112, Reward: -34.83621277199833, Epsilon: 0.4067310063530636\n",
      "Episode: 113, Reward: -248.1112536391121, Epsilon: 0.40347715830223907\n",
      "Episode: 114, Reward: -47.682532699224915, Epsilon: 0.40024934103582116\n",
      "Episode: 115, Reward: -81.08927318653522, Epsilon: 0.3970473463075346\n",
      "Episode: 116, Reward: 110.80767167605997, Epsilon: 0.3938709675370743\n",
      "Episode: 117, Reward: 86.62521212581336, Epsilon: 0.3907199997967777\n",
      "Episode: 118, Reward: 104.70967177706858, Epsilon: 0.38759423979840346\n",
      "Episode: 119, Reward: -100.00685129211277, Epsilon: 0.38449348588001625\n",
      "Episode: 120, Reward: -30.312193778027535, Epsilon: 0.3814175379929761\n",
      "Episode: 121, Reward: -95.77563358822951, Epsilon: 0.3783661976890323\n",
      "Episode: 122, Reward: -218.52112105364353, Epsilon: 0.37533926810752005\n",
      "Episode: 123, Reward: -152.5929304108114, Epsilon: 0.3723365539626599\n",
      "Episode: 124, Reward: -159.89628091036187, Epsilon: 0.3693578615309586\n",
      "Episode: 125, Reward: -17.51346377111102, Epsilon: 0.36640299863871095\n",
      "Episode: 126, Reward: -291.88687864943824, Epsilon: 0.3634717746496013\n",
      "Episode: 127, Reward: -142.26522731332605, Epsilon: 0.3605640004524045\n",
      "Episode: 128, Reward: -122.40951116982974, Epsilon: 0.3576794884487853\n",
      "Episode: 129, Reward: -70.25063298816836, Epsilon: 0.354818052541195\n",
      "Episode: 130, Reward: -64.80567871612004, Epsilon: 0.3519795081208654\n",
      "Episode: 131, Reward: -201.41718322477936, Epsilon: 0.3491636720558985\n",
      "Episode: 132, Reward: -262.92196956939677, Epsilon: 0.3463703626794513\n",
      "Episode: 133, Reward: -243.75653072892558, Epsilon: 0.3435993997780157\n",
      "Episode: 134, Reward: -170.657030806004, Epsilon: 0.34085060457979155\n",
      "Episode: 135, Reward: -66.79760457500662, Epsilon: 0.3381237997431532\n",
      "Episode: 136, Reward: -163.0966721703353, Epsilon: 0.33541880934520796\n",
      "Episode: 137, Reward: 12.114746407616778, Epsilon: 0.3327354588704463\n",
      "Episode: 138, Reward: -210.80906582820558, Epsilon: 0.3300735751994827\n",
      "Episode: 139, Reward: -74.68465541838029, Epsilon: 0.32743298659788683\n",
      "Episode: 140, Reward: -60.65061809511511, Epsilon: 0.3248135227051037\n",
      "Episode: 141, Reward: -199.8549572913034, Epsilon: 0.3222150145234629\n",
      "Episode: 142, Reward: -34.20414060830235, Epsilon: 0.3196372944072752\n",
      "Episode: 143, Reward: -209.69707900416824, Epsilon: 0.317080196052017\n",
      "Episode: 144, Reward: -142.81029431719188, Epsilon: 0.31454355448360083\n",
      "Episode: 145, Reward: -60.838084780786204, Epsilon: 0.31202720604773204\n",
      "Episode: 146, Reward: -58.99861692078048, Epsilon: 0.30953098839935017\n",
      "Episode: 147, Reward: -253.05370090538355, Epsilon: 0.30705474049215536\n",
      "Episode: 148, Reward: -37.62003229243207, Epsilon: 0.3045983025682181\n",
      "Episode: 149, Reward: -104.28254129894778, Epsilon: 0.3021615161476724\n",
      "Episode: 150, Reward: -192.5221601949093, Epsilon: 0.299744224018491\n",
      "Episode: 151, Reward: -41.335529355230314, Epsilon: 0.2973462702263431\n",
      "Episode: 152, Reward: -88.27509750647144, Epsilon: 0.2949675000645323\n",
      "Episode: 153, Reward: -237.77082807536527, Epsilon: 0.29260776006401606\n",
      "Episode: 154, Reward: -231.41883270612078, Epsilon: 0.2902668979835039\n",
      "Episode: 155, Reward: 130.65576730644074, Epsilon: 0.2879447627996359\n",
      "Episode: 156, Reward: -5.586408945394567, Epsilon: 0.2856412046972388\n",
      "Episode: 157, Reward: -284.2962585537229, Epsilon: 0.2833560750596609\n",
      "Episode: 158, Reward: -48.496620990163606, Epsilon: 0.2810892264591836\n",
      "Episode: 159, Reward: -81.32370651063874, Epsilon: 0.27884051264751014\n",
      "Episode: 160, Reward: 94.37739707488228, Epsilon: 0.27660978854633006\n",
      "Episode: 161, Reward: -162.4049689417316, Epsilon: 0.2743969102379594\n",
      "Episode: 162, Reward: -37.78563597137045, Epsilon: 0.27220173495605576\n",
      "Episode: 163, Reward: 67.25522768105655, Epsilon: 0.27002412107640733\n",
      "Episode: 164, Reward: 18.1223730797885, Epsilon: 0.2678639281077961\n",
      "Episode: 165, Reward: -185.6443342416712, Epsilon: 0.2657210166829337\n",
      "Episode: 166, Reward: 13.360364707874055, Epsilon: 0.2635952485494702\n",
      "Episode: 167, Reward: -72.60237275548383, Epsilon: 0.2614864865610744\n",
      "Episode: 168, Reward: -79.76669674519214, Epsilon: 0.25939459466858583\n",
      "Episode: 169, Reward: -57.609753563862185, Epsilon: 0.2573194379112371\n",
      "Episode: 170, Reward: -1.1526377939752734, Epsilon: 0.25526088240794725\n",
      "Episode: 171, Reward: -80.52170150997065, Epsilon: 0.2532187953486837\n",
      "Episode: 172, Reward: -70.2431367196414, Epsilon: 0.25119304498589423\n",
      "Episode: 173, Reward: -10.21510371334, Epsilon: 0.24918350062600708\n",
      "Episode: 174, Reward: -214.47201482131447, Epsilon: 0.24719003262099903\n",
      "Episode: 175, Reward: 107.65447678298158, Epsilon: 0.24521251236003103\n",
      "Episode: 176, Reward: -128.3803256449094, Epsilon: 0.24325081226115078\n",
      "Episode: 177, Reward: 13.899134950511723, Epsilon: 0.24130480576306157\n",
      "Episode: 178, Reward: -36.01147676592703, Epsilon: 0.23937436731695708\n",
      "Episode: 179, Reward: -287.34262443858984, Epsilon: 0.2374593723784214\n",
      "Episode: 180, Reward: -59.22759935260797, Epsilon: 0.23555969739939403\n",
      "Episode: 181, Reward: -230.9240122305011, Epsilon: 0.23367521982019887\n",
      "Episode: 182, Reward: -44.04615016361681, Epsilon: 0.23180581806163728\n",
      "Episode: 183, Reward: -174.42448378189005, Epsilon: 0.22995137151714418\n",
      "Episode: 184, Reward: -40.1784159666246, Epsilon: 0.228111760545007\n",
      "Episode: 185, Reward: -232.3969196421158, Epsilon: 0.22628686646064694\n",
      "Episode: 186, Reward: -34.370573380020645, Epsilon: 0.22447657152896178\n",
      "Episode: 187, Reward: -245.19908396640827, Epsilon: 0.2226807589567301\n",
      "Episode: 188, Reward: -305.1676803693741, Epsilon: 0.22089931288507625\n",
      "Episode: 189, Reward: -18.06889027096196, Epsilon: 0.21913211838199564\n",
      "Episode: 190, Reward: -58.912212506029206, Epsilon: 0.21737906143493968\n",
      "Episode: 191, Reward: -14.376183721407386, Epsilon: 0.21564002894346015\n",
      "Episode: 192, Reward: 90.19209843398097, Epsilon: 0.21391490871191246\n",
      "Episode: 193, Reward: -28.195019074207295, Epsilon: 0.21220358944221715\n",
      "Episode: 194, Reward: -101.08301127017802, Epsilon: 0.2105059607266794\n",
      "Episode: 195, Reward: -45.252844395894286, Epsilon: 0.20882191304086598\n",
      "Episode: 196, Reward: -24.796551105957363, Epsilon: 0.20715133773653904\n",
      "Episode: 197, Reward: -73.47133985413211, Epsilon: 0.20549412703464673\n",
      "Episode: 198, Reward: -213.60738677304766, Epsilon: 0.20385017401836955\n",
      "Episode: 199, Reward: -206.28820850526228, Epsilon: 0.20221937262622258\n",
      "Episode: 200, Reward: -84.50970678010947, Epsilon: 0.2006016176452128\n",
      "Episode: 201, Reward: 16.724968816091774, Epsilon: 0.1989968047040511\n",
      "Episode: 202, Reward: 19.921087403040175, Epsilon: 0.1974048302664187\n",
      "Episode: 203, Reward: 61.02208456682784, Epsilon: 0.19582559162428734\n",
      "Episode: 204, Reward: -181.29656756695925, Epsilon: 0.19425898689129303\n",
      "Episode: 205, Reward: -195.66396977689845, Epsilon: 0.19270491499616269\n",
      "Episode: 206, Reward: 253.5215266444717, Epsilon: 0.19116327567619337\n",
      "Episode: 207, Reward: -178.4278211359624, Epsilon: 0.18963396947078381\n",
      "Episode: 208, Reward: 8.899475150449206, Epsilon: 0.18811689771501755\n",
      "Episode: 209, Reward: 95.61612469783944, Epsilon: 0.1866119625332974\n",
      "Episode: 210, Reward: -39.985269432092636, Epsilon: 0.18511906683303103\n",
      "Episode: 211, Reward: -26.977659306791438, Epsilon: 0.1836381142983668\n",
      "Episode: 212, Reward: -54.61726490240859, Epsilon: 0.18216900938397987\n",
      "Episode: 213, Reward: -167.24218626067756, Epsilon: 0.18071165730890804\n",
      "Episode: 214, Reward: 32.673908935245464, Epsilon: 0.17926596405043677\n",
      "Episode: 215, Reward: -261.67611604099335, Epsilon: 0.17783183633803326\n",
      "Episode: 216, Reward: 3.5557447495438055, Epsilon: 0.176409181647329\n",
      "Episode: 217, Reward: 88.33561378182475, Epsilon: 0.17499790819415037\n",
      "Episode: 218, Reward: -159.99010091836544, Epsilon: 0.17359792492859716\n",
      "Episode: 219, Reward: 3.7255419337312645, Epsilon: 0.17220914152916839\n",
      "Episode: 220, Reward: -225.24351731297804, Epsilon: 0.17083146839693503\n",
      "Episode: 221, Reward: -16.894258436395617, Epsilon: 0.16946481664975954\n",
      "Episode: 222, Reward: 34.74477201071238, Epsilon: 0.16810909811656147\n",
      "Episode: 223, Reward: 193.7445773016475, Epsilon: 0.16676422533162896\n",
      "Episode: 224, Reward: 178.74647245982334, Epsilon: 0.16543011152897594\n",
      "Episode: 225, Reward: 260.72472462793667, Epsilon: 0.16410667063674414\n",
      "Episode: 226, Reward: 188.262047488856, Epsilon: 0.16279381727165018\n",
      "Episode: 227, Reward: -151.33069096631118, Epsilon: 0.16149146673347697\n",
      "Episode: 228, Reward: -258.00427054893004, Epsilon: 0.16019953499960915\n",
      "Episode: 229, Reward: -216.04356882625098, Epsilon: 0.15891793871961227\n",
      "Episode: 230, Reward: -51.87371875692023, Epsilon: 0.15764659520985536\n",
      "Episode: 231, Reward: -200.1088473672446, Epsilon: 0.15638542244817652\n",
      "Episode: 232, Reward: -12.467073274380354, Epsilon: 0.15513433906859111\n",
      "Episode: 233, Reward: -10.076912439018258, Epsilon: 0.1538932643560424\n",
      "Episode: 234, Reward: -229.78978092957266, Epsilon: 0.15266211824119405\n",
      "Episode: 235, Reward: -175.53245480731272, Epsilon: 0.1514408212952645\n",
      "Episode: 236, Reward: -19.881682089909773, Epsilon: 0.1502292947249024\n",
      "Episode: 237, Reward: -85.36992062750113, Epsilon: 0.14902746036710318\n",
      "Episode: 238, Reward: -3.7414558119892405, Epsilon: 0.14783524068416634\n",
      "Episode: 239, Reward: -208.91005300084652, Epsilon: 0.146652558758693\n",
      "Episode: 240, Reward: 166.00201697282137, Epsilon: 0.14547933828862347\n",
      "Episode: 241, Reward: 105.21145170085111, Epsilon: 0.14431550358231449\n",
      "Episode: 242, Reward: -9.944508188969294, Epsilon: 0.14316097955365598\n",
      "Episode: 243, Reward: -19.957192425949103, Epsilon: 0.14201569171722672\n",
      "Episode: 244, Reward: -0.2898707860240961, Epsilon: 0.1408795661834889\n",
      "Episode: 245, Reward: -202.23406600975508, Epsilon: 0.139752529654021\n",
      "Episode: 246, Reward: 125.35097865295916, Epsilon: 0.13863450941678881\n",
      "Episode: 247, Reward: -181.6345182351962, Epsilon: 0.1375254333414545\n",
      "Episode: 248, Reward: -148.99748970369623, Epsilon: 0.13642522987472286\n",
      "Episode: 249, Reward: -258.2698923084737, Epsilon: 0.13533382803572508\n",
      "Episode: 250, Reward: 247.05653845814592, Epsilon: 0.13425115741143928\n",
      "Episode: 251, Reward: -14.154335964560289, Epsilon: 0.13317714815214776\n",
      "Episode: 252, Reward: -159.1805010600237, Epsilon: 0.13211173096693057\n",
      "Episode: 253, Reward: -17.19291662918816, Epsilon: 0.13105483711919513\n",
      "Episode: 254, Reward: 180.42741833484854, Epsilon: 0.13000639842224157\n",
      "Episode: 255, Reward: -207.88654448865293, Epsilon: 0.12896634723486364\n",
      "Episode: 256, Reward: -58.47219518929305, Epsilon: 0.12793461645698473\n",
      "Episode: 257, Reward: -44.701136349696654, Epsilon: 0.12691113952532884\n",
      "Episode: 258, Reward: -225.93993342785453, Epsilon: 0.1258958504091262\n",
      "Episode: 259, Reward: -40.271118565633245, Epsilon: 0.1248886836058532\n",
      "Episode: 260, Reward: -3.169540030297867, Epsilon: 0.12388957413700638\n",
      "Episode: 261, Reward: 201.24459293485606, Epsilon: 0.12289845754391032\n",
      "Episode: 262, Reward: 2.88574433502896, Epsilon: 0.12191526988355904\n",
      "Episode: 263, Reward: -207.26990482441016, Epsilon: 0.12093994772449057\n",
      "Episode: 264, Reward: -103.62095035097448, Epsilon: 0.11997242814269464\n",
      "Episode: 265, Reward: -8.462231318765191, Epsilon: 0.11901264871755309\n",
      "Episode: 266, Reward: -30.590281722422603, Epsilon: 0.11806054752781266\n",
      "Episode: 267, Reward: -74.09395059656964, Epsilon: 0.11711606314759015\n",
      "Episode: 268, Reward: -91.97057172147878, Epsilon: 0.11617913464240943\n",
      "Episode: 269, Reward: -302.3176863167906, Epsilon: 0.11524970156527016\n",
      "Episode: 270, Reward: -22.829186658759312, Epsilon: 0.114327703952748\n",
      "Episode: 271, Reward: -60.73899832854124, Epsilon: 0.11341308232112601\n",
      "Episode: 272, Reward: -51.38703379772156, Epsilon: 0.11250577766255701\n",
      "Episode: 273, Reward: -29.189027772284362, Epsilon: 0.11160573144125656\n",
      "Episode: 274, Reward: 208.3375410578141, Epsilon: 0.1107128855897265\n",
      "Episode: 275, Reward: -50.699150911019686, Epsilon: 0.10982718250500868\n",
      "Episode: 276, Reward: 163.2081642504277, Epsilon: 0.10894856504496861\n",
      "Episode: 277, Reward: -78.16204522534468, Epsilon: 0.10807697652460886\n",
      "Episode: 278, Reward: -274.97875072167596, Epsilon: 0.10721236071241198\n",
      "Episode: 279, Reward: -1.0711277392938143, Epsilon: 0.10635466182671269\n",
      "Episode: 280, Reward: -32.014447750274314, Epsilon: 0.10550382453209899\n",
      "Episode: 281, Reward: 267.40804308716184, Epsilon: 0.1046597939358422\n",
      "Episode: 282, Reward: -77.2075396820373, Epsilon: 0.10382251558435546\n",
      "Episode: 283, Reward: 223.47986537885367, Epsilon: 0.1029919354596806\n",
      "Episode: 284, Reward: -208.21012887337162, Epsilon: 0.10216799997600316\n",
      "Episode: 285, Reward: 245.36124073849737, Epsilon: 0.10135065597619514\n",
      "Episode: 286, Reward: 228.38765074532864, Epsilon: 0.10053985072838557\n",
      "Episode: 287, Reward: -49.923260894309166, Epsilon: 0.09973553192255849\n",
      "Episode: 288, Reward: -37.154161451048395, Epsilon: 0.09893764766717802\n",
      "Episode: 289, Reward: -15.83627162506815, Epsilon: 0.0981461464858406\n",
      "Episode: 290, Reward: -60.63353603463044, Epsilon: 0.09736097731395386\n",
      "Episode: 291, Reward: -2.4511487149019473, Epsilon: 0.09658208949544224\n",
      "Episode: 292, Reward: -225.3659883478698, Epsilon: 0.0958094327794787\n",
      "Episode: 293, Reward: -215.747619952486, Epsilon: 0.09504295731724287\n",
      "Episode: 294, Reward: -198.11816281252834, Epsilon: 0.09428261365870493\n",
      "Episode: 295, Reward: 2.742179122297429, Epsilon: 0.09352835274943529\n",
      "Episode: 296, Reward: -235.42131256796452, Epsilon: 0.09278012592743981\n",
      "Episode: 297, Reward: -25.497936600646938, Epsilon: 0.0920378849200203\n",
      "Episode: 298, Reward: -186.83310007112533, Epsilon: 0.09130158184066013\n",
      "Episode: 299, Reward: -20.99867069766094, Epsilon: 0.09057116918593484\n",
      "Episode: 300, Reward: -16.8888228944854, Epsilon: 0.08984659983244736\n",
      "Episode: 301, Reward: 233.06391284613935, Epsilon: 0.08912782703378778\n",
      "Episode: 302, Reward: -73.17606356295539, Epsilon: 0.08841480441751748\n",
      "Episode: 303, Reward: -22.562082428152166, Epsilon: 0.08770748598217734\n",
      "Episode: 304, Reward: -31.65497859456282, Epsilon: 0.08700582609431992\n",
      "Episode: 305, Reward: -17.769272314031426, Epsilon: 0.08630977948556535\n",
      "Episode: 306, Reward: -54.05024582091012, Epsilon: 0.08561930124968083\n",
      "Episode: 307, Reward: -194.741860236759, Epsilon: 0.08493434683968339\n",
      "Episode: 308, Reward: -223.64963913653452, Epsilon: 0.08425487206496592\n",
      "Episode: 309, Reward: -74.66696465877195, Epsilon: 0.08358083308844619\n",
      "Episode: 310, Reward: -203.50775675691307, Epsilon: 0.08291218642373861\n",
      "Episode: 311, Reward: -30.574591751410182, Epsilon: 0.0822488889323487\n",
      "Episode: 312, Reward: -165.48281803451624, Epsilon: 0.08159089782088992\n",
      "Episode: 313, Reward: -38.29568061472969, Epsilon: 0.0809381706383228\n",
      "Episode: 314, Reward: -18.315497181509656, Epsilon: 0.08029066527321622\n",
      "Episode: 315, Reward: -199.98616140025098, Epsilon: 0.07964833995103049\n",
      "Episode: 316, Reward: -51.99900973075839, Epsilon: 0.07901115323142224\n",
      "Episode: 317, Reward: 244.14474910468465, Epsilon: 0.07837906400557086\n",
      "Episode: 318, Reward: 4.039937382054916, Epsilon: 0.07775203149352629\n",
      "Episode: 319, Reward: -79.2954066946489, Epsilon: 0.07713001524157807\n",
      "Episode: 320, Reward: -222.47873477700847, Epsilon: 0.07651297511964544\n",
      "Episode: 321, Reward: -210.72687726756828, Epsilon: 0.07590087131868828\n",
      "Episode: 322, Reward: -30.243574778002767, Epsilon: 0.07529366434813878\n",
      "Episode: 323, Reward: -30.657720762256403, Epsilon: 0.07469131503335366\n",
      "Episode: 324, Reward: -200.5516186476972, Epsilon: 0.07409378451308683\n",
      "Episode: 325, Reward: -16.729606933966338, Epsilon: 0.07350103423698214\n",
      "Episode: 326, Reward: -159.78189350263162, Epsilon: 0.07291302596308628\n",
      "Episode: 327, Reward: 219.44542224754696, Epsilon: 0.07232972175538159\n",
      "Episode: 328, Reward: -253.44493549174666, Epsilon: 0.07175108398133853\n",
      "Episode: 329, Reward: -192.29254564791967, Epsilon: 0.07117707530948782\n",
      "Episode: 330, Reward: 187.48860306271257, Epsilon: 0.07060765870701191\n",
      "Episode: 331, Reward: -217.99375370055094, Epsilon: 0.07004279743735582\n",
      "Episode: 332, Reward: -14.014339666375832, Epsilon: 0.06948245505785698\n",
      "Episode: 333, Reward: 177.45766673948833, Epsilon: 0.06892659541739413\n",
      "Episode: 334, Reward: -40.92306683490855, Epsilon: 0.06837518265405497\n",
      "Episode: 335, Reward: -258.8655866515697, Epsilon: 0.06782818119282252\n",
      "Episode: 336, Reward: -194.10329303163226, Epsilon: 0.06728555574327995\n",
      "Episode: 337, Reward: -31.157125487114172, Epsilon: 0.06674727129733371\n",
      "Episode: 338, Reward: 16.38410561943283, Epsilon: 0.06621329312695504\n",
      "Episode: 339, Reward: -219.83136041984898, Epsilon: 0.06568358678193939\n",
      "Episode: 340, Reward: -240.42675611878474, Epsilon: 0.06515811808768388\n",
      "Episode: 341, Reward: -149.2136313287239, Epsilon: 0.0646368531429824\n",
      "Episode: 342, Reward: 7.255657626448624, Epsilon: 0.06411975831783853\n",
      "Episode: 343, Reward: -202.32610005764957, Epsilon: 0.06360680025129582\n",
      "Episode: 344, Reward: 178.42060173449676, Epsilon: 0.06309794584928545\n",
      "Episode: 345, Reward: -183.55318390739387, Epsilon: 0.06259316228249116\n",
      "Episode: 346, Reward: 40.489767664263866, Epsilon: 0.062092416984231236\n",
      "Episode: 347, Reward: -18.609060356060183, Epsilon: 0.06159567764835738\n",
      "Episode: 348, Reward: 77.50381089841571, Epsilon: 0.06110291222717052\n",
      "Episode: 349, Reward: -235.36501306547947, Epsilon: 0.06061408892935315\n",
      "Episode: 350, Reward: -246.80660936595575, Epsilon: 0.06012917621791833\n",
      "Episode: 351, Reward: -83.696385041331, Epsilon: 0.05964814280817498\n",
      "Episode: 352, Reward: -178.57678522886476, Epsilon: 0.05917095766570958\n",
      "Episode: 353, Reward: 7.235462004901464, Epsilon: 0.058697590004383904\n",
      "Episode: 354, Reward: -18.133503281835004, Epsilon: 0.05822800928434883\n",
      "Episode: 355, Reward: -9.203653216948595, Epsilon: 0.05776218521007404\n",
      "Episode: 356, Reward: -60.30576288762491, Epsilon: 0.05730008772839345\n",
      "Episode: 357, Reward: 239.37058750387334, Epsilon: 0.0568416870265663\n",
      "Episode: 358, Reward: -259.18600342168975, Epsilon: 0.05638695353035377\n",
      "Episode: 359, Reward: 253.3926711811189, Epsilon: 0.05593585790211094\n",
      "Episode: 360, Reward: -198.70941327305985, Epsilon: 0.05548837103889405\n",
      "Episode: 361, Reward: -202.3894517641616, Epsilon: 0.055044464070582895\n",
      "Episode: 362, Reward: -3.385653299366652, Epsilon: 0.05460410835801823\n",
      "Episode: 363, Reward: -16.35244066898734, Epsilon: 0.054167275491154084\n",
      "Episode: 364, Reward: -9.736345919608837, Epsilon: 0.05373393728722485\n",
      "Episode: 365, Reward: -199.51275057488584, Epsilon: 0.05330406578892705\n",
      "Episode: 366, Reward: 16.757609972338997, Epsilon: 0.05287763326261564\n",
      "Episode: 367, Reward: -229.0203901412513, Epsilon: 0.05245461219651471\n",
      "Episode: 368, Reward: 3.266398643919615, Epsilon: 0.0520349752989426\n",
      "Episode: 369, Reward: -21.567152708712783, Epsilon: 0.051618695496551056\n",
      "Episode: 370, Reward: -181.51333531145184, Epsilon: 0.05120574593257865\n",
      "Episode: 371, Reward: -180.90129674489043, Epsilon: 0.05079609996511802\n",
      "Episode: 372, Reward: 247.09626233890475, Epsilon: 0.05038973116539707\n",
      "Episode: 373, Reward: 207.9429285833194, Epsilon: 0.049986613316073895\n",
      "Episode: 374, Reward: -166.28714048627785, Epsilon: 0.0495867204095453\n",
      "Episode: 375, Reward: -204.3804055223128, Epsilon: 0.04919002664626894\n",
      "Episode: 376, Reward: -203.3876905903795, Epsilon: 0.04879650643309879\n",
      "Episode: 377, Reward: -17.08860671713495, Epsilon: 0.048406134381634\n",
      "Episode: 378, Reward: -201.47730721329407, Epsilon: 0.048018885306580925\n",
      "Episode: 379, Reward: -214.35651226290366, Epsilon: 0.047634734224128276\n",
      "Episode: 380, Reward: -199.43972899553745, Epsilon: 0.04725365635033525\n",
      "Episode: 381, Reward: -62.310133906079145, Epsilon: 0.04687562709953257\n",
      "Episode: 382, Reward: -47.80276665968104, Epsilon: 0.04650062208273631\n",
      "Episode: 383, Reward: -204.80691123140397, Epsilon: 0.04612861710607442\n",
      "Episode: 384, Reward: -152.7273338382734, Epsilon: 0.04575958816922582\n",
      "Episode: 385, Reward: -192.17237381435217, Epsilon: 0.04539351146387201\n",
      "Episode: 386, Reward: -146.72023621611032, Epsilon: 0.04503036337216103\n",
      "Episode: 387, Reward: -12.085529741827159, Epsilon: 0.04467012046518374\n",
      "Episode: 388, Reward: -103.81407114358987, Epsilon: 0.04431275950146227\n",
      "Episode: 389, Reward: -220.09084717178868, Epsilon: 0.04395825742545057\n",
      "Episode: 390, Reward: -12.349232559871524, Epsilon: 0.043606591366046964\n",
      "Episode: 391, Reward: -192.12737837883628, Epsilon: 0.04325773863511859\n",
      "Episode: 392, Reward: 11.001510789786138, Epsilon: 0.042911676726037636\n",
      "Episode: 393, Reward: -270.44758207332086, Epsilon: 0.042568383312229334\n",
      "Episode: 394, Reward: -188.98378269800526, Epsilon: 0.0422278362457315\n",
      "Episode: 395, Reward: 223.59155627012856, Epsilon: 0.04189001355576565\n",
      "Episode: 396, Reward: -186.915439475023, Epsilon: 0.041554893447319524\n",
      "Episode: 397, Reward: 226.66022388098315, Epsilon: 0.04122245429974097\n",
      "Episode: 398, Reward: -19.30342184525354, Epsilon: 0.04089267466534304\n",
      "Episode: 399, Reward: -189.57881486090935, Epsilon: 0.040565533268020294\n",
      "Episode: 400, Reward: -185.90729343027255, Epsilon: 0.04024100900187613\n",
      "Episode: 401, Reward: 8.918812123772199, Epsilon: 0.03991908092986112\n",
      "Episode: 402, Reward: -66.08861476310054, Epsilon: 0.03959972828242223\n",
      "Episode: 403, Reward: -27.396648683491065, Epsilon: 0.03928293045616285\n",
      "Episode: 404, Reward: -226.64894524647985, Epsilon: 0.03896866701251355\n",
      "Episode: 405, Reward: -174.61622618916948, Epsilon: 0.038656917676413445\n",
      "Episode: 406, Reward: -32.43899113391559, Epsilon: 0.038347662335002135\n",
      "Episode: 407, Reward: -220.12188131142085, Epsilon: 0.03804088103632212\n",
      "Episode: 408, Reward: -172.05101834656645, Epsilon: 0.03773655398803154\n",
      "Episode: 409, Reward: 256.31363780562145, Epsilon: 0.037434661556127284\n",
      "Episode: 410, Reward: 232.81153143008882, Epsilon: 0.03713518426367827\n",
      "Episode: 411, Reward: -90.86483316168447, Epsilon: 0.036838102789568845\n",
      "Episode: 412, Reward: -190.50084004548503, Epsilon: 0.0365433979672523\n",
      "Episode: 413, Reward: -193.3126794708811, Epsilon: 0.03625105078351428\n",
      "Episode: 414, Reward: -44.71053017477141, Epsilon: 0.035961042377246163\n",
      "Episode: 415, Reward: -56.24585631015937, Epsilon: 0.035673354038228196\n",
      "Episode: 416, Reward: 230.65640011018877, Epsilon: 0.03538796720592237\n",
      "Episode: 417, Reward: 227.0187866487525, Epsilon: 0.03510486346827499\n",
      "Episode: 418, Reward: -195.58277318192188, Epsilon: 0.034824024560528785\n",
      "Episode: 419, Reward: -187.7617893215546, Epsilon: 0.03454543236404455\n",
      "Episode: 420, Reward: -29.393062836018046, Epsilon: 0.0342690689051322\n",
      "Episode: 421, Reward: -212.86561138409752, Epsilon: 0.03399491635389114\n",
      "Episode: 422, Reward: -71.60673590635108, Epsilon: 0.03372295702306001\n",
      "Episode: 423, Reward: 141.95093518107538, Epsilon: 0.03345317336687553\n",
      "Episode: 424, Reward: -81.94116306504134, Epsilon: 0.03318554797994053\n",
      "Episode: 425, Reward: -220.0599848958617, Epsilon: 0.032920063596101\n",
      "Episode: 426, Reward: -172.97034791090857, Epsilon: 0.03265670308733219\n",
      "Episode: 427, Reward: -25.916705014856234, Epsilon: 0.032395449462633535\n",
      "Episode: 428, Reward: -222.58557123899973, Epsilon: 0.032136285866932464\n",
      "Episode: 429, Reward: -56.64025254388968, Epsilon: 0.031879195579997\n",
      "Episode: 430, Reward: 222.53464173071035, Epsilon: 0.031624162015357025\n",
      "Episode: 431, Reward: -29.30020837975694, Epsilon: 0.03137116871923417\n",
      "Episode: 432, Reward: -194.92462122359507, Epsilon: 0.031120199369480298\n",
      "Episode: 433, Reward: -178.424625687657, Epsilon: 0.030871237774524454\n",
      "Episode: 434, Reward: -2.1437684106132195, Epsilon: 0.03062426787232826\n",
      "Episode: 435, Reward: -27.330498009282763, Epsilon: 0.03037927372934963\n",
      "Episode: 436, Reward: -200.0068801632269, Epsilon: 0.030136239539514834\n",
      "Episode: 437, Reward: -78.65159141147298, Epsilon: 0.029895149623198714\n",
      "Episode: 438, Reward: -211.10091464918486, Epsilon: 0.029655988426213125\n",
      "Episode: 439, Reward: -6.448893676851924, Epsilon: 0.02941874051880342\n",
      "Episode: 440, Reward: -38.1811801876072, Epsilon: 0.02918339059465299\n",
      "Episode: 441, Reward: 41.430912348758795, Epsilon: 0.028949923469895767\n",
      "Episode: 442, Reward: -61.199113719934545, Epsilon: 0.0287183240821366\n",
      "Episode: 443, Reward: 239.00415522136987, Epsilon: 0.028488577489479507\n",
      "Episode: 444, Reward: -58.84926999951752, Epsilon: 0.02826066886956367\n",
      "Episode: 445, Reward: 259.4960525972948, Epsilon: 0.02803458351860716\n",
      "Episode: 446, Reward: 7.585176846169816, Epsilon: 0.0278103068504583\n",
      "Episode: 447, Reward: 214.82696349534598, Epsilon: 0.027587824395654634\n",
      "Episode: 448, Reward: -160.2805293389127, Epsilon: 0.027367121800489398\n",
      "Episode: 449, Reward: 224.98443795566612, Epsilon: 0.027148184826085484\n",
      "Episode: 450, Reward: -190.54311521316907, Epsilon: 0.026930999347476798\n",
      "Episode: 451, Reward: -175.09258792299858, Epsilon: 0.026715551352696983\n",
      "Episode: 452, Reward: 257.43080300097665, Epsilon: 0.026501826941875407\n",
      "Episode: 453, Reward: -243.27002205237358, Epsilon: 0.026289812326340402\n",
      "Episode: 454, Reward: -15.809136017866251, Epsilon: 0.02607949382772968\n",
      "Episode: 455, Reward: -232.8121515113181, Epsilon: 0.025870857877107842\n",
      "Episode: 456, Reward: -22.926338040109968, Epsilon: 0.02566389101409098\n",
      "Episode: 457, Reward: -211.47147165964253, Epsilon: 0.02545857988597825\n",
      "Episode: 458, Reward: -181.60857972920962, Epsilon: 0.025254911246890426\n",
      "Episode: 459, Reward: 189.94428874642296, Epsilon: 0.0250528719569153\n",
      "Episode: 460, Reward: -198.66454674874126, Epsilon: 0.02485244898125998\n",
      "Episode: 461, Reward: -219.67841106931016, Epsilon: 0.0246536293894099\n",
      "Episode: 462, Reward: -274.1011072065595, Epsilon: 0.02445640035429462\n",
      "Episode: 463, Reward: -28.458446269416044, Epsilon: 0.024260749151460263\n",
      "Episode: 464, Reward: -185.91090731517585, Epsilon: 0.02406666315824858\n",
      "Episode: 465, Reward: -18.59366831525263, Epsilon: 0.02387412985298259\n",
      "Episode: 466, Reward: -34.54799050547622, Epsilon: 0.02368313681415873\n",
      "Episode: 467, Reward: -195.0333343130569, Epsilon: 0.023493671719645462\n",
      "Episode: 468, Reward: -214.50817988577677, Epsilon: 0.023305722345888298\n",
      "Episode: 469, Reward: 250.46315252470117, Epsilon: 0.02311927656712119\n",
      "Episode: 470, Reward: -15.203140862535832, Epsilon: 0.022934322354584223\n",
      "Episode: 471, Reward: 249.60400843530755, Epsilon: 0.02275084777574755\n",
      "Episode: 472, Reward: 247.10259067152737, Epsilon: 0.02256884099354157\n",
      "Episode: 473, Reward: -226.01511976997472, Epsilon: 0.02238829026559324\n",
      "Episode: 474, Reward: 220.7028719289957, Epsilon: 0.022209183943468495\n",
      "Episode: 475, Reward: -43.108568600547144, Epsilon: 0.022031510471920746\n",
      "Episode: 476, Reward: -405.7391878318584, Epsilon: 0.02185525838814538\n",
      "Episode: 477, Reward: -40.25502151137613, Epsilon: 0.021680416321040216\n",
      "Episode: 478, Reward: -53.37164422362687, Epsilon: 0.021506972990471895\n",
      "Episode: 479, Reward: -192.85698295330909, Epsilon: 0.02133491720654812\n",
      "Episode: 480, Reward: -157.75065968456656, Epsilon: 0.021164237868895736\n",
      "Episode: 481, Reward: -210.9609324761208, Epsilon: 0.02099492396594457\n",
      "Episode: 482, Reward: 218.28069456993504, Epsilon: 0.020826964574217014\n",
      "Episode: 483, Reward: 20.59669162467422, Epsilon: 0.020660348857623276\n",
      "Episode: 484, Reward: -126.97290794143797, Epsilon: 0.02049506606676229\n",
      "Episode: 485, Reward: -201.5841864859812, Epsilon: 0.02033110553822819\n",
      "Episode: 486, Reward: -180.98293668335302, Epsilon: 0.020168456693922365\n",
      "Episode: 487, Reward: -253.0496891003213, Epsilon: 0.020007109040370986\n",
      "Episode: 488, Reward: 249.01096332173475, Epsilon: 0.019847052168048017\n",
      "Episode: 489, Reward: 174.8878951125038, Epsilon: 0.019688275750703633\n",
      "Episode: 490, Reward: -212.92950354350864, Epsilon: 0.019530769544698002\n",
      "Episode: 491, Reward: 236.03293398154526, Epsilon: 0.019374523388340417\n",
      "Episode: 492, Reward: -192.48112871959643, Epsilon: 0.019219527201233693\n",
      "Episode: 493, Reward: -36.90174077230512, Epsilon: 0.019065770983623824\n",
      "Episode: 494, Reward: 38.06112555287439, Epsilon: 0.018913244815754834\n",
      "Episode: 495, Reward: 185.8500133706861, Epsilon: 0.018761938857228797\n",
      "Episode: 496, Reward: -25.2955091761432, Epsilon: 0.018611843346370966\n",
      "Episode: 497, Reward: -208.7649116800568, Epsilon: 0.018462948599599998\n",
      "Episode: 498, Reward: -25.566044461641027, Epsilon: 0.0183152450108032\n",
      "Episode: 499, Reward: -34.11677870165768, Epsilon: 0.018168723050716772\n",
      "Episode: 500, Reward: -224.143621819077, Epsilon: 0.018023373266311038\n"
     ]
    }
   ],
   "source": [
    "my_agent=Agent(is_train=True) \n",
    "\n",
    "train_rewards=my_agent.run(is_train=True,render=False,episodes=500,DDQN=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4abde09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: -36.28427794377227, Epsilon: 0.01\n",
      "Episode: 2, Reward: -39.310861964776635, Epsilon: 0.01\n",
      "Episode: 3, Reward: -202.94240142334243, Epsilon: 0.01\n",
      "Episode: 4, Reward: -33.028158457987416, Epsilon: 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m test_rewards=\u001b[43mmy_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mAgent.run\u001b[39m\u001b[34m(self, is_train, render, episodes, DDQN)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     57\u001b[39m         action=policy_net(state).argmax().item()\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m new_state,reward_move,term,trunc,_=\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m reward_ep+=reward_move\n\u001b[32m     61\u001b[39m done=term \u001b[38;5;129;01mor\u001b[39;00m trunc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vithu\\OneDrive\\Desktop\\Data Science and ML\\Reinforcement Learning\\Midterm Submission\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vithu\\OneDrive\\Desktop\\Data Science and ML\\Reinforcement Learning\\Midterm Submission\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vithu\\OneDrive\\Desktop\\Data Science and ML\\Reinforcement Learning\\Midterm Submission\\.venv\\Lib\\site-packages\\gymnasium\\core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vithu\\OneDrive\\Desktop\\Data Science and ML\\Reinforcement Learning\\Midterm Submission\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vithu\\OneDrive\\Desktop\\Data Science and ML\\Reinforcement Learning\\Midterm Submission\\.venv\\Lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:665\u001b[39m, in \u001b[36mLunarLander.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    662\u001b[39m     reward = +\u001b[32m100\u001b[39m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(state, dtype=np.float32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vithu\\OneDrive\\Desktop\\Data Science and ML\\Reinforcement Learning\\Midterm Submission\\.venv\\Lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:778\u001b[39m, in \u001b[36mLunarLander.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    776\u001b[39m     \u001b[38;5;28mself\u001b[39m.screen.blit(\u001b[38;5;28mself\u001b[39m.surf, (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m))\n\u001b[32m    777\u001b[39m     pygame.event.pump()\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrender_fps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m     pygame.display.flip()\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mrgb_array\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "test_rewards=my_agent.run(is_train=False,render=True,episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254200df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(my_agent.policy_net.state_dict(),\"lunarlander_dqn.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
